{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    # x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 256, 'BLOCK_SIZE_E': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 256, 'BLOCK_SIZE_E': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 128, 'BLOCK_SIZE_E': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 64, 'BLOCK_SIZE_E': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 128, 'BLOCK_SIZE_E': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 32, 'BLOCK_SIZE_E': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 32, 'BLOCK_SIZE_E': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_D': 64, 'BLOCK_SIZE_E': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n",
    "    ],\n",
    "    key=['B', 'D', 'E'],\n",
    ")\n",
    "@triton.jit\n",
    "def mlp_kernel(\n",
    "    # Pointers to matrices\n",
    "    x_ptr, w1_ptr, b1_ptr, w2_ptr, b2_ptr, z_ptr, o_ptr,\n",
    "    # Matrix dimensions\n",
    "    B, D, E,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_xb, stride_xd,\n",
    "    stride_w1d, stride_w1e,\n",
    "    stride_b1e,\n",
    "    stride_w2e, stride_w2d,\n",
    "    stride_b2d,\n",
    "    stride_zb, stride_ze,\n",
    "    stride_ob, stride_od,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_D: tl.constexpr, BLOCK_SIZE_E: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    ACTIVATION: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the mlp\n",
    "    Z = X @ W1 + b1, H = f(Z), O = H @ W2 + b2.\n",
    "    - X has shape (B, D),\n",
    "    - W1 has shape (D, E), b1 has shape (E)\n",
    "    - W2 has shape (E, D), b2 has shape (D)\n",
    "    - Z has shape (B, E), H has shape (B, E)\n",
    "    - O has shape (B, D)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    pid_b = tl.program_id(axis=0)\n",
    "    pid_d = tl.program_id(axis=1)\n",
    "    TARGET_TYPE = x_ptr.type.element_ty\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    o = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_D), dtype=tl.float32)\n",
    "    for e in range(0, tl.cdiv(E, BLOCK_SIZE_E)):\n",
    "        z = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_E), dtype=tl.float32)\n",
    "        # loop over D\n",
    "        x_ptrs = tl.make_block_ptr(\n",
    "            base=x_ptr,\n",
    "            shape=(B, D),\n",
    "            strides=(stride_xb, stride_xd),\n",
    "            offsets=(pid_b * BLOCK_SIZE_B, 0),\n",
    "            block_shape=(BLOCK_SIZE_B, BLOCK_SIZE_K),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        w1_ptrs = tl.make_block_ptr(\n",
    "            base=w1_ptr,\n",
    "            shape=(D, E),\n",
    "            strides=(stride_w1d, stride_w1e),\n",
    "            offsets=(0, e * BLOCK_SIZE_E),\n",
    "            block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_E),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        for k in range(0, tl.cdiv(D, BLOCK_SIZE_K)):\n",
    "            x = tl.load(x_ptrs)\n",
    "            w1 = tl.load(w1_ptrs)\n",
    "            z = tl.dot(x, w1, z)\n",
    "            x_ptrs = tl.advance(x_ptrs, (0, BLOCK_SIZE_K))\n",
    "            w1_ptrs = tl.advance(w1_ptrs, (BLOCK_SIZE_K, 0))\n",
    "        # add bias\n",
    "        b1_ptrs = tl.make_block_ptr(\n",
    "            base=b1_ptr,\n",
    "            shape=(E,),\n",
    "            strides=(stride_b1e,),\n",
    "            offsets=(e * BLOCK_SIZE_E,),\n",
    "            block_shape=(BLOCK_SIZE_E,),\n",
    "            order=(0,),\n",
    "        )\n",
    "        b1 = tl.load(b1_ptrs)\n",
    "        z = z + b1\n",
    "        z = z.to(TARGET_TYPE)\n",
    "        # store z\n",
    "        z_ptrs = tl.make_block_ptr(\n",
    "            base=z_ptr,\n",
    "            shape=(B, E),\n",
    "            strides=(stride_zb, stride_ze),\n",
    "            offsets=(pid_b * BLOCK_SIZE_B, e * BLOCK_SIZE_E),\n",
    "            block_shape=(BLOCK_SIZE_B, BLOCK_SIZE_E),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        tl.store(z_ptrs, z)\n",
    "        # You can fuse arbitrary activation functions here\n",
    "        h = z\n",
    "        if ACTIVATION == \"leaky_relu\":\n",
    "            h = leaky_relu(z).to(TARGET_TYPE)\n",
    "        # loop over W2\n",
    "        o_d = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_D), dtype=tl.float32)\n",
    "        w2_ptrs = tl.make_block_ptr(\n",
    "            base=w2_ptr,\n",
    "            shape=(E, D),\n",
    "            strides=(stride_w2e, stride_w2d),\n",
    "            offsets=(e * BLOCK_SIZE_E, pid_d * BLOCK_SIZE_D),\n",
    "            block_shape=(BLOCK_SIZE_E, BLOCK_SIZE_D),\n",
    "            order=(1, 0),\n",
    "        )\n",
    "        w2 = tl.load(w2_ptrs)\n",
    "        # tl.static_print(h)\n",
    "        # tl.static_print(w2)\n",
    "        o = tl.dot(h, w2, o)\n",
    "        tl.static_print('o', o)\n",
    "    # add bias\n",
    "    b2_ptrs = tl.make_block_ptr(\n",
    "        base=b2_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(stride_b2d,),\n",
    "        offsets=(pid_d * BLOCK_SIZE_D,),\n",
    "        block_shape=(BLOCK_SIZE_D,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    b2 = tl.load(b2_ptrs)\n",
    "    o = o + b2\n",
    "    o = o.to(TARGET_TYPE)\n",
    "    # store o\n",
    "    o_ptrs = tl.make_block_ptr(\n",
    "        base=o_ptr,\n",
    "        shape=(B, D),\n",
    "        strides=(stride_ob, stride_od),\n",
    "        offsets=(pid_b * BLOCK_SIZE_B, pid_d * BLOCK_SIZE_D),\n",
    "        block_shape=(BLOCK_SIZE_B, BLOCK_SIZE_D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    tl.store(o_ptrs, o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 256, 'BLOCK_SIZE_E': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 256, 'BLOCK_SIZE_E': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 128, 'BLOCK_SIZE_E': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 64, 'BLOCK_SIZE_E': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 128, 'BLOCK_SIZE_E': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_D': 32, 'BLOCK_SIZE_E': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_D': 32, 'BLOCK_SIZE_E': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_D': 64, 'BLOCK_SIZE_E': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n",
    "    ],\n",
    "    key=['B', 'D', 'E'],\n",
    ")\n",
    "@triton.jit\n",
    "def mlp_simple_kernel(\n",
    "    # Pointers to matrices\n",
    "    x_ptr, w1_ptr, b1_ptr, w2_ptr, b2_ptr, z_ptr, o_ptr,\n",
    "    # Matrix dimensions\n",
    "    B, D: tl.constexpr, E,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_xb, stride_xd,\n",
    "    stride_w1d, stride_w1e,\n",
    "    stride_b1e,\n",
    "    stride_w2e, stride_w2d,\n",
    "    stride_b2d,\n",
    "    stride_zb, stride_ze,\n",
    "    stride_ob, stride_od,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_D: tl.constexpr, BLOCK_SIZE_E: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    ACTIVATION: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the mlp\n",
    "    Z = X @ W1 + b1, H = f(Z), O = H @ W2 + b2.\n",
    "    - X has shape (B, D),\n",
    "    - W1 has shape (D, E), b1 has shape (E)\n",
    "    - W2 has shape (E, D), b2 has shape (D)\n",
    "    - Z has shape (B, E), H has shape (B, E)\n",
    "    - O has shape (B, D)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_b = pid\n",
    "    # tl.static_print(x_ptr.dtype)\n",
    "    # tl.static_print(x_ptr.__dict__)\n",
    "    # tl.static_print(x_ptr.dtype.__dict__)\n",
    "    # tl.static_print(x_ptr.dtype.element_ty)\n",
    "    TARGET_TYPE = x_ptr.dtype.element_ty\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    offs_b_blk = (pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B))\n",
    "    offs_d = tl.arange(0, D)\n",
    "    # x_ptrs = x_ptr + (offs_b_blk[:, None] * stride_xb + offs_d[None, :] * stride_xd)\n",
    "    # x = tl.load(x_ptrs, mask=(offs_b_blk[:, None] < B) and (offs_d[None, :] < D), other=0.0)\n",
    "    # tl.static_print(x_ptrs)\n",
    "    # tl.static_print(w1_ptrs)\n",
    "    # tl.static_print(b1_ptrs)\n",
    "    # tl.static_print(w2_ptrs)\n",
    "    # tl.static_print(b2_ptrs)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    o = tl.zeros((BLOCK_SIZE_B, 0), dtype=tl.float32)\n",
    "    o_d = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_D), dtype=tl.float32)\n",
    "    for e in range(0, tl.cdiv(E, BLOCK_SIZE_E)):\n",
    "        z = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_E), dtype=tl.float32)\n",
    "        # loop over D\n",
    "        offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "        offs_b_blk = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n",
    "        offs_e_blk = e * BLOCK_SIZE_E + tl.arange(0, BLOCK_SIZE_E)\n",
    "        x_ptrs = x_ptr + (offs_b_blk[:, None] * stride_xb + offs_k[None, :] * stride_xd)\n",
    "        w1_ptrs = w1_ptr + (offs_k[:, None] * stride_w1d + offs_e_blk[None, :] * stride_w1e)\n",
    "        for k in range(0, tl.cdiv(D, BLOCK_SIZE_K)):\n",
    "            x = tl.load(x_ptrs, mask=(offs_b_blk[:, None] < B) & (offs_k[None, :] + k * BLOCK_SIZE_K < D), other=0.0)\n",
    "            w1 = tl.load(w1_ptrs, mask=(offs_k[:, None] + k * BLOCK_SIZE_K < D) & (offs_e_blk[None, :] < E), other=0.0)\n",
    "            z = tl.dot(x, w1, z)\n",
    "            x_ptrs += BLOCK_SIZE_K * stride_xd\n",
    "            w1_ptrs += BLOCK_SIZE_K * stride_w1d\n",
    "        # add bias\n",
    "        b1_ptrs = b1_ptr + (offs_e_blk[:] * stride_b1e)\n",
    "        b1 = tl.load(b1_ptrs, mask=(offs_e_blk[:] < E), other=0.0)\n",
    "        z = z + b1[None, :]\n",
    "        z = z.to(TARGET_TYPE)\n",
    "        # store z\n",
    "        offs_z_blk = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n",
    "        z_ptrs = z_ptr + (offs_z_blk[:, None] * stride_zb) + (offs_e_blk[None, :] * stride_ze)\n",
    "        tl.store(z_ptrs, z, mask=(offs_b_blk[:, None] < B) and (offs_e_blk[None, :] < E))\n",
    "        # You can fuse arbitrary activation functions here\n",
    "        h = z\n",
    "        if ACTIVATION == \"leaky_relu\":\n",
    "            h = leaky_relu(z)\n",
    "            h = h.to(TARGET_TYPE)\n",
    "        # loop over W2\n",
    "        # for k in range(0, tl.cdiv(D, BLOCK_SIZE_K)):\n",
    "        #     offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "        #     w2_ptrs = w2_ptr + (offs_e_blk[:, None] * stride_w2e + offs_k[None, :] * stride_w2d)\n",
    "        #     w2 = tl.load(w2_ptrs, mask=(offs_e_blk[:, None] < E) & (offs_k[None, :] + k * BLOCK_SIZE_K < D), other=0.0)\n",
    "        #     o[:, offs_k] = tl.dot(h, w2, o[:, offs_k])\n",
    "        w2_ptrs = w2_ptr + (offs_e_blk[:, None] * stride_w2e + offs_d[None, :] * stride_w2d)\n",
    "        w2 = tl.load(w2_ptrs, mask=(offs_e_blk[:, None] < E) & (offs_d[None, :] < D), other=0.0)\n",
    "        o = tl.dot(h, w2, o)\n",
    "    # add bias\n",
    "    b2_ptrs = b2_ptr + (offs_d[:] * stride_b2d)\n",
    "    b2 = tl.load(b2_ptrs, mask=(offs_d[:] < D), other=0.0)\n",
    "    o = o + b2[None, :]\n",
    "    o = o.to(TARGET_TYPE)\n",
    "    # store o\n",
    "    offs_o_blk = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)\n",
    "    o_ptrs = o_ptr + (offs_o_blk[:, None] * stride_ob + offs_d[None, :] * stride_od)\n",
    "    tl.store(o_ptrs, o, mask=(offs_o_blk[:, None] < B) & (offs_d[None, :] < D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_triton(x, w1, b1, w2, b2, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert x.shape[1] == w1.shape[0], \"Incompatible dimensions\"\n",
    "    assert w1.shape[1] == w2.shape[0], \"Incompatible dimensions\"\n",
    "    assert w1.shape[1] == b1.shape[0], \"Incompatible dimensions\"\n",
    "    assert w2.shape[1] == b2.shape[0], \"Incompatible dimensions\"\n",
    "    assert w2.shape[1] == b2.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix X must be contiguous\"\n",
    "    assert w1.is_contiguous(), \"Matrix W1 must be contiguous\"\n",
    "    assert b1.is_contiguous(), \"Matrix B1 must be contiguous\"\n",
    "    assert w2.is_contiguous(), \"Matrix W2 must be contiguous\"\n",
    "    assert b2.is_contiguous(), \"Matrix B2 must be contiguous\"\n",
    "    B, D = x.shape\n",
    "    E = w1.shape[1]\n",
    "\n",
    "    # Allocates output.\n",
    "    z = torch.empty((B, E), device=x.device, dtype=x.dtype)\n",
    "    o = torch.empty((B, D), device=x.device, dtype=x.dtype)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(B, META['BLOCK_SIZE_B']),\n",
    "        triton.cdiv(D, META['BLOCK_SIZE_D']),\n",
    "    )\n",
    "    mlp_kernel[grid](\n",
    "        x, w1, b1, w2, b2, z, o,\n",
    "        B, D, E,\n",
    "        x.stride(0), x.stride(1),\n",
    "        w1.stride(0), w1.stride(1),\n",
    "        b1.stride(0),\n",
    "        w2.stride(0), w2.stride(1),\n",
    "        b2.stride(0),\n",
    "        z.stride(0), z.stride(1),\n",
    "        o.stride(0), o.stride(1),\n",
    "        ACTIVATION=activation\n",
    "    )\n",
    "    return o, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_torch(x, w1, b1, w2, b2, activation=\"\"):\n",
    "    z = torch.matmul(x, w1) + b1\n",
    "    if activation == \"leaky_relu\":\n",
    "        z = torch.nn.functional.leaky_relu(z)\n",
    "    o = torch.matmul(z, w2) + b2\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output=(tensor([[  286.0000,   328.0000,  -159.0000,  ..., -1224.0000,   242.0000,\n",
      "           162.0000],\n",
      "        [  888.0000,   584.0000,    23.7500,  ...,   219.0000,   428.0000,\n",
      "           580.0000],\n",
      "        [   20.7500,   940.0000,  -282.0000,  ...,   173.0000,   824.0000,\n",
      "          1424.0000],\n",
      "        ...,\n",
      "        [   49.7500,   676.0000,  -370.0000,  ...,  -580.0000,  -440.0000,\n",
      "           416.0000],\n",
      "        [  552.0000,  -274.0000,  -444.0000,  ...,  -728.0000,   -19.8750,\n",
      "           692.0000],\n",
      "        [  968.0000,  1256.0000,   418.0000,  ...,  -524.0000,  -432.0000,\n",
      "           394.0000]], device='cuda:0', dtype=torch.bfloat16), torch.Size([256, 768]))\n",
      "torch_output=(tensor([[  286.0000,   328.0000,  -160.0000,  ..., -1224.0000,   242.0000,\n",
      "           161.0000],\n",
      "        [  888.0000,   584.0000,    23.7500,  ...,   220.0000,   428.0000,\n",
      "           580.0000],\n",
      "        [   20.8750,   940.0000,  -282.0000,  ...,   173.0000,   824.0000,\n",
      "          1424.0000],\n",
      "        ...,\n",
      "        [   49.7500,   676.0000,  -370.0000,  ...,  -576.0000,  -440.0000,\n",
      "           418.0000],\n",
      "        [  552.0000,  -276.0000,  -444.0000,  ...,  -728.0000,   -19.7500,\n",
      "           696.0000],\n",
      "        [  968.0000,  1256.0000,   418.0000,  ...,  -524.0000,  -432.0000,\n",
      "           396.0000]], device='cuda:0', dtype=torch.bfloat16), torch.Size([256, 768]))\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "def unit_test_simple():\n",
    "    torch.manual_seed(125)\n",
    "    dtype = torch.bfloat16\n",
    "    x = torch.randn((256, 768), device='cuda', dtype=dtype)\n",
    "    w1 = torch.randn((768, 1024), device='cuda', dtype=dtype)\n",
    "    b1 = torch.zeros(1024, device='cuda', dtype=dtype)\n",
    "    w2 = torch.randn((1024, 768), device='cuda', dtype=dtype)\n",
    "    b2 = torch.randn(768, device='cuda', dtype=dtype)\n",
    "    triton_output = mlp_triton(x, w1, b1, w2, b2, activation=\"leaky_relu\")\n",
    "    torch_output = mlp_torch(x, w1, b1, w2, b2, activation=\"leaky_relu\")\n",
    "    print(f\"triton_output={triton_output[0], triton_output[0].shape}\")\n",
    "    print(f\"torch_output={torch_output, torch_output.shape}\")\n",
    "    if torch.allclose(triton_output[0], torch_output, atol=1e-2, rtol=1e-2):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "unit_test_simple()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['B', 'D', 'E'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[\n",
    "            64 * i for i in range(2, 32)\n",
    "        ],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['torch', 'triton'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"Torch\", \"Triton\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '-'), ('blue', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"mlp-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    )\n",
    ")\n",
    "def benchmark(B, D, E, provider):\n",
    "    dtype = torch.bfloat16\n",
    "    x = torch.randn((B, D), device='cuda', dtype=dtype)\n",
    "    w1 = torch.randn((D, E), device='cuda', dtype=dtype)\n",
    "    b1 = torch.zeros(E, device='cuda', dtype=dtype)\n",
    "    w2 = torch.randn((E, D), device='cuda', dtype=dtype)\n",
    "    b2 = torch.randn(D, device='cuda', dtype=dtype)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider.startswith('torch'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: mlp_torch(x, w1, b1, w2, b2, activation=\"leaky_relu\"), quantiles=quantiles)\n",
    "    if provider.startswith('triton'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: mlp_simple(x, w1, b1, w2, b2, activation=\"leaky_relu\"), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * B * D * E * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_simple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/triton/testing.py:349\u001b[0m, in \u001b[0;36mMark.run\u001b[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<html><body>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bench \u001b[38;5;129;01min\u001b[39;00m benchmarks:\n\u001b[0;32m--> 349\u001b[0m     result_dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[1;32m    351\u001b[0m         html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image src=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbench\u001b[38;5;241m.\u001b[39mplot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/triton/testing.py:292\u001b[0m, in \u001b[0;36mMark._run\u001b[0;34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[0m\n\u001b[1;32m    290\u001b[0m row_mean, row_min, row_max \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m bench\u001b[38;5;241m.\u001b[39mline_vals:\n\u001b[0;32m--> 292\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_arg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwrags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         y_mean, y_min, y_max \u001b[38;5;241m=\u001b[39m ret\n",
      "Cell \u001b[0;32mIn[64], line 30\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(B, D, E, provider)\u001b[0m\n\u001b[1;32m     28\u001b[0m     ms, min_ms, max_ms \u001b[38;5;241m=\u001b[39m triton\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mdo_bench(\u001b[38;5;28;01mlambda\u001b[39;00m: mlp_torch(x, w1, b1, w2, b2, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaky_relu\u001b[39m\u001b[38;5;124m\"\u001b[39m), quantiles\u001b[38;5;241m=\u001b[39mquantiles)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriton\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     ms, min_ms, max_ms \u001b[38;5;241m=\u001b[39m \u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaky_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m perf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ms: \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m B \u001b[38;5;241m*\u001b[39m D \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-12\u001b[39m \u001b[38;5;241m/\u001b[39m (ms \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perf(ms), perf(max_ms), perf(min_ms)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/triton/testing.py:106\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode, device_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    104\u001b[0m di \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdevice_interface\u001b[38;5;241m.\u001b[39mget_interface_for_device(device_type)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m di\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# We maintain a buffer of 256 MB that we clear\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# before each kernel call to make sure that the L2\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# doesn't contain any input data before the run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 30\u001b[0m, in \u001b[0;36mbenchmark.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     ms, min_ms, max_ms \u001b[38;5;241m=\u001b[39m triton\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mdo_bench(\u001b[38;5;28;01mlambda\u001b[39;00m: mlp_torch(x, w1, b1, w2, b2, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaky_relu\u001b[39m\u001b[38;5;124m\"\u001b[39m), quantiles\u001b[38;5;241m=\u001b[39mquantiles)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriton\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     ms, min_ms, max_ms \u001b[38;5;241m=\u001b[39m triton\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mdo_bench(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mmlp_simple\u001b[49m(x, w1, b1, w2, b2, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaky_relu\u001b[39m\u001b[38;5;124m\"\u001b[39m), quantiles\u001b[38;5;241m=\u001b[39mquantiles)\n\u001b[1;32m     31\u001b[0m perf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ms: \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m B \u001b[38;5;241m*\u001b[39m D \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-12\u001b[39m \u001b[38;5;241m/\u001b[39m (ms \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perf(ms), perf(max_ms), perf(min_ms)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_simple' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark.run(show_plots=True, print_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
