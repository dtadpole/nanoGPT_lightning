{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    # x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K', 'float32', 'allow_tf32']\n",
    ")\n",
    "@triton.jit\n",
    "def cvmm_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am, stride_ak,\n",
    "    stride_bo, stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    stride_index, stride_sel, stride_out_index,\n",
    "    out_index_is_none: tl.constexpr,\n",
    "    float32: tl.constexpr, allow_tf32: tl.constexpr,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "\n",
    "    sel_first = tl.load(sel_ptr + pid_m * BLOCK_SIZE_M * stride_sel)\n",
    "    sel_last = tl.load(sel_ptr + (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) * stride_sel)\n",
    "    sel_all = tl.load(sel_ptr + stride_sel * ((pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M))\n",
    "\n",
    "    for matrix_id in range(sel_first, sel_last + 1):\n",
    "        # ----------------------------------------------------------\n",
    "        # Create pointers for the first blocks of A and B.\n",
    "        # We will advance this pointer as we move in the K direction\n",
    "        # and accumulate\n",
    "        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "        # See above `Pointer Arithmetics` section for details\n",
    "        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "\n",
    "        remap_offs_am = tl.load(index_ptr + stride_index * offs_am)\n",
    "\n",
    "        # Create offset pointers\n",
    "        offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "        a_ptrs = a_ptr + (remap_offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "        b_ptrs = b_ptr + matrix_id * stride_bo + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Iterate to compute a block of the C matrix.\n",
    "        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "        # of fp32 values for higher accuracy.\n",
    "        # `accumulator` will be converted back to fp16 after the loop.\n",
    "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "            # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "            # If it is out of bounds, set it to 0.\n",
    "            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "            # We accumulate along the K dimension.\n",
    "\n",
    "            if not float32:\n",
    "                a = a.to(tl.float16)\n",
    "                b = b.to(tl.float16)\n",
    "\n",
    "            accumulator += tl.dot(a, b, allow_tf32=allow_tf32)\n",
    "\n",
    "            # Advance the ptrs to the next K block.\n",
    "            a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "            b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "\n",
    "        if not float32:\n",
    "            c = accumulator.to(tl.float16)\n",
    "        else:\n",
    "            c = accumulator\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Write back the block of the output matrix C with masks.\n",
    "        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "\n",
    "        if out_index_is_none:\n",
    "            remap_offs_cm = remap_offs_am\n",
    "        else:\n",
    "            remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)\n",
    "\n",
    "        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "        c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "        c_mask = ((offs_cm[:, None] < M) & (sel_all[:, None] == matrix_id)) & (offs_cn[None, :] < N)\n",
    "        tl.store(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 4}, num_stages=4, num_warps=4),\n",
    "\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 128}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),\n",
    "\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 16}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 64}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 8, 'K_BLOCKS': 32}, num_stages=4, num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K', 'float32_out', 'allow_tf32', 'op_float16'], reset_to_zero = ['c_ptr']\n",
    ")\n",
    "@triton.jit\n",
    "def cvmm_backward_kernel3(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_co, stride_cm, stride_cn,\n",
    "    stride_index, stride_sel, stride_out_index,\n",
    "    out_index_is_none: tl.constexpr,\n",
    "    float32_out: tl.constexpr, allow_tf32: tl.constexpr, op_float16: tl.constexpr,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr, K_BLOCKS: tl.constexpr\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    k_block_id = tl.program_id(axis=1)\n",
    "\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetics` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "\n",
    "    a_ptrs_this = a_ptr + offs_am[:, None] * stride_am\n",
    "    b_ptrs_this = b_ptr + offs_bn[None, :] * stride_bn\n",
    "\n",
    "    # Kactual = end_i - start_i\n",
    "    # Nblocks = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K\n",
    "\n",
    "    # WORK_PER_WORKER = (Nblocks + K_BLOCKS - 1) // K_BLOCKS\n",
    "    # WORK_PER_WORKER = WORK_PER_WORKER if WORK_PER_WORKER > MIN_WORK_SIZE else MIN_WORK_SIZE\n",
    "\n",
    "\n",
    "    # # Kloop_start = (Kactual + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K\n",
    "\n",
    "    # first_block_k = k_block_id * WORK_PER_WORKER\n",
    "    # last_block_k = min((k_block_id+1) * WORK_PER_WORKER, Nblocks)\n",
    "\n",
    "    block_start_index = k_block_id * BLOCK_SIZE_K * K_BLOCKS\n",
    "    block_end_index = min(block_start_index + BLOCK_SIZE_K * K_BLOCKS, K) - 1\n",
    "\n",
    "    first_mat = tl.load(sel_ptr + stride_sel * block_start_index)\n",
    "    last_mat = tl.load(sel_ptr + stride_sel * block_end_index)\n",
    "\n",
    "\n",
    "    for matrix_index in range(first_mat, last_mat + 1):\n",
    "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "\n",
    "        start_i = block_start_index\n",
    "        end_i = block_end_index + 1\n",
    "        while start_i < end_i:\n",
    "            middle = (start_i + end_i) // 2\n",
    "            middle_matrix = tl.load(sel_ptr + middle * stride_sel)\n",
    "            if middle_matrix < matrix_index:\n",
    "                start_i = middle + 1\n",
    "            else:\n",
    "                end_i = middle\n",
    "\n",
    "\n",
    "        # # Continue binary search: find the first matrix that is > matrix_index\n",
    "        start_i2 = start_i\n",
    "        end_i = block_end_index + 1\n",
    "        while start_i2 < end_i:\n",
    "            middle = (start_i2 + end_i) // 2\n",
    "            middle_matrix = tl.load(sel_ptr + middle * stride_sel)\n",
    "            if middle_matrix <= matrix_index:\n",
    "                start_i2 = middle + 1\n",
    "            else:\n",
    "                end_i = middle\n",
    "\n",
    "        end_i = start_i2\n",
    "\n",
    "        count = end_i - start_i\n",
    "\n",
    "        block_mem_indices_f_base = start_i  + tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "        if count > 0:\n",
    "            for k in range((count + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K):\n",
    "                # block_mem_indices = (k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K\n",
    "                block_mem_indices_f = block_mem_indices_f_base + k * BLOCK_SIZE_K\n",
    "                block_mem_indices = block_mem_indices_f % K\n",
    "                a_index = tl.load(index_ptr + stride_index * block_mem_indices)\n",
    "                if out_index_is_none:\n",
    "                    b_index = a_index\n",
    "                else:\n",
    "                    b_index = tl.load(out_index_ptr + stride_out_index * block_mem_indices)\n",
    "                sel_ok = block_mem_indices_f < end_i\n",
    "\n",
    "                a_ptrs = a_ptrs_this + a_index[None, :] * stride_ak\n",
    "                b_ptrs = b_ptrs_this + b_index[:, None] * stride_bk\n",
    "\n",
    "                # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "                # If it is out of bounds, set it to 0.\n",
    "                a = tl.load(a_ptrs, mask=sel_ok[None, :], other=0.0)\n",
    "                b = tl.load(b_ptrs, mask=sel_ok[:, None], other=0.0)\n",
    "\n",
    "                if op_float16:\n",
    "                    a = a.to(tl.float16)\n",
    "                    b = b.to(tl.float16)\n",
    "\n",
    "                # We accumulate along the K dimension.\n",
    "                accumulator += tl.dot(a, b, allow_tf32=allow_tf32)\n",
    "\n",
    "            if float32_out:\n",
    "                c = accumulator\n",
    "            else:\n",
    "                c = accumulator.to(tl.float16)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Write back the block of the output matrix C with masks.\n",
    "            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "            c_ptrs = c_ptr + stride_co * matrix_index + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "            # tl.store(c_ptrs, c, mask=c_mask)\n",
    "            tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This is not allowed since there's already a kernel registered from python overriding cvmm_triton's behavior for CompositeExplicitAutograd dispatch key and mylib namespace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.library.define(\"mylib::cvmm_triton\", \"(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmylib::cvmm_triton\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mcvmm_triton\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msel_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/torch/library.py:442\u001b[0m, in \u001b[0;36mimpl.<locals>.register\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    440\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m--> 442\u001b[0m     \u001b[43muse_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/torch/library.py:234\u001b[0m, in \u001b[0;36mLibrary.impl\u001b[0;34m(self, op_name, fn, dispatch_key, with_keyset)\u001b[0m\n\u001b[1;32m    230\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dispatch_key\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m _impls:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# TODO: in future, add more info about where the existing function is registered (this info is\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# today already returned by the C++ warning when impl is called but we error out before that)\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is not allowed since there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms already a kernel registered from python overriding \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms behavior for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dispatch key and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m namespace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    236\u001b[0m                        \u001b[38;5;28mformat\u001b[39m(name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dispatch_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns))\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    239\u001b[0m     dispatcher_op_name \u001b[38;5;241m=\u001b[39m name\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This is not allowed since there's already a kernel registered from python overriding cvmm_triton's behavior for CompositeExplicitAutograd dispatch key and mylib namespace."
     ]
    }
   ],
   "source": [
    "\n",
    "# torch.library.define(\"mylib::cvmm_triton\", \"(Tensor x, Tensor sel_index, Tensor sel, Tensor keys, ScalarType out_dtype, Tensor out_index) -> Tensor\")\n",
    "\n",
    "@torch.library.impl(\"mylib::cvmm_triton\", \"default\")\n",
    "def cvmm_triton(\n",
    "    x: torch.Tensor,\n",
    "    sel_index: torch.Tensor,\n",
    "    sel: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    out_dtype: torch.dtype,\n",
    "    out_index: torch.Tensor\n",
    "):\n",
    "    x = x.flatten(end_dim=-2)\n",
    "    assert x.shape[-1] == keys.shape[1]\n",
    "\n",
    "    sel_shape = sel.shape\n",
    "    sel = sel.flatten()\n",
    "\n",
    "    M = sel.shape[0]\n",
    "    O, K, N = keys.shape\n",
    "    # Allocates output.\n",
    "    out = torch.empty((M, N), device=x.device, dtype=out_dtype)\n",
    "    # out = torch.zeros((M, N), device=x.device, dtype=out_dtype)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "\n",
    "    # expected_m_per_matrix = int(math.ceil(M / O * 1.5))\n",
    "    # expected_m_per_matrix = M\n",
    "\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "    )\n",
    "\n",
    "    out_index_is_none = False\n",
    "    if out_index.numel() == 1 and out_index == -1:\n",
    "        out_index_is_none = True\n",
    "\n",
    "    cvmm_kernel[grid](\n",
    "        x, keys, out, sel_index, sel, out_index,\n",
    "        M, N, K,\n",
    "        x.stride(0), x.stride(1),\n",
    "        keys.stride(0), keys.stride(1), keys.stride(2),\n",
    "        out.stride(0), out.stride(1),\n",
    "        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),\n",
    "        out_index_is_none=out_index_is_none,\n",
    "        float32=out.dtype==torch.float32, allow_tf32=False, #torch.backends.cuda.matmul.allow_tf32\n",
    "    )\n",
    "\n",
    "    return out.view(*sel_shape, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.library.impl_abstract(\"mylib::cvmm_triton\", cvmm_triton)\n",
    "# def cvmm_triton_abstract(x, sel_idx, sel, keys, out_dtype, out_index):\n",
    "#     sel_shape = sel.shape\n",
    "#     sel = sel.flatten()\n",
    "#     M = sel.shape[0]\n",
    "#     O, K, N = keys.shape\n",
    "#     out = torch.empty((M, N), device=x.device, dtype=out_dtype)\n",
    "#     sel_shape = sel.shape\n",
    "#     return out.view(*sel_shape, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.library.impl(\"mylib::cvmm_triton_backward\", \"default\")\n",
    "def cvmm_triton_backward(\n",
    "    x: torch.Tensor,\n",
    "    sel_index: torch.Tensor,\n",
    "    sel: torch.Tensor,\n",
    "    grads: torch.Tensor,\n",
    "    n_experts: int,\n",
    "    key_dtype: torch.dtype,\n",
    "    op_float16: bool,\n",
    "    out_index: torch.Tensor\n",
    "):\n",
    "    x = x.flatten(end_dim=-2)\n",
    "    x = x.transpose(0, 1)\n",
    "    grads = grads.flatten(end_dim=-2)\n",
    "    sel = sel.flatten()\n",
    "    M, _ = x.shape\n",
    "    K, N = grads.shape\n",
    "    out = torch.zeros((n_experts, M, N), device=x.device, dtype=key_dtype)\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), triton.cdiv(K, META['BLOCK_SIZE_K'] * META['K_BLOCKS'])\n",
    "    )\n",
    "    out_index_is_none = False\n",
    "    if out_index.numel() == 1 and out_index == -1:\n",
    "        out_index_is_none = True\n",
    "\n",
    "    cvmm_backward_kernel3[grid](\n",
    "        x, grads, out, sel_index, sel, out_index,\n",
    "        M, N, K,\n",
    "        x.stride(0), x.stride(1),\n",
    "        grads.stride(0), grads.stride(1),\n",
    "        out.stride(0), out.stride(1), out.stride(2),\n",
    "        sel_index.stride(0), sel.stride(0), 0 if out_index_is_none else out_index.stride(0),\n",
    "        out_index_is_none=out_index_is_none,\n",
    "        float32_out=out.dtype == torch.float32,\n",
    "        op_float16=op_float16,\n",
    "        allow_tf32=False #torch.backends.cuda.matmul.allow_tf32\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVMM(torch.autograd.Function):\n",
    "    warned = False\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x: torch.Tensor,\n",
    "        sel_index: torch.Tensor,\n",
    "        sel: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        out_index: Optional[torch.Tensor] = None,\n",
    "        reduction_weight: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        ctx.save_for_backward(x, keys, sel, sel_index, out_index, reduction_weight)\n",
    "        out_type = torch.float16 if torch.is_autocast_enabled() else x.dtype\n",
    "        if out_index is None:\n",
    "            out_index = torch.tensor(-1).cuda()\n",
    "\n",
    "        res = torch.ops.mylib.cvmm_triton(x, sel_index, sel, keys, out_type, out_index)\n",
    "\n",
    "        if reduction_weight is not None:\n",
    "            res = res.view(*reduction_weight.shape, res.shape[-1])\n",
    "            res = (reduction_weight.unsqueeze(-2).type_as(res) @ res).squeeze(-2)\n",
    "\n",
    "        ctx.op_type = out_type\n",
    "        ctx.keys_type = keys.dtype\n",
    "        ctx.is_autocast = torch.is_autocast_enabled()\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, keys, sel, sel_index, out_index, reduction_weight = ctx.saved_tensors\n",
    "        keys_dt = keys\n",
    "\n",
    "        # Backward for weight\n",
    "        if reduction_weight is not None:\n",
    "            # Project back the grads with he reduction weight, so the grad for the weight matrix is ok\n",
    "            grad_output_w = reduction_weight.unsqueeze(-1).type_as(grad_output) @ grad_output.unsqueeze(-2)\n",
    "        else:\n",
    "            grad_output_w  = grad_output\n",
    "\n",
    "        out_index_is_none = False\n",
    "        if out_index is None:\n",
    "            out_index_is_none = True\n",
    "            out_index = torch.tensor(-1).cuda()\n",
    "\n",
    "        grad_w = cvmm_triton_backward(\n",
    "            x,\n",
    "            sel_index,\n",
    "            sel,\n",
    "            grad_output_w,\n",
    "            keys_dt.shape[0],\n",
    "            ctx.keys_type,\n",
    "            ctx.is_autocast,\n",
    "            out_index=out_index\n",
    "        )\n",
    "\n",
    "        # Backward for input and reduction weight\n",
    "        grad_w_off = None\n",
    "\n",
    "        bw_index = sel_index if out_index_is_none else out_index\n",
    "        bw_index_out = torch.tensor(-1).cuda()\n",
    "        if reduction_weight is not None:\n",
    "            # Hack the output indices to emulate repeats\n",
    "            bw_index_out = bw_index\n",
    "            bw_index = bw_index // reduction_weight.shape[-1]\n",
    "\n",
    "        grad_x_full = torch.ops.mylib.cvmm_triton(\n",
    "            grad_output,\n",
    "            bw_index,\n",
    "            sel,\n",
    "            keys_dt.transpose(1,2),\n",
    "            ctx.op_type,\n",
    "            bw_index_out\n",
    "        )\n",
    "\n",
    "        grad_x_full = grad_x_full.view(*x.shape[:-1], -1, x.shape[-1])\n",
    "        if reduction_weight is not None:\n",
    "            # grad_x_full is the unscaled grad. For the input, we have to scale it, for the reduction wegiht,\n",
    "            # we have to compute dot products with the input.\n",
    "            grad_x = (reduction_weight.view(*grad_x_full.shape[:-1]).unsqueeze(-2).type_as(grad_x_full) @ grad_x_full).squeeze(-2)\n",
    "            grad_w_off = (grad_x_full.type_as(reduction_weight) @ x.unsqueeze(-1).type_as(reduction_weight)).squeeze(-1).view_as(reduction_weight)\n",
    "        elif grad_x_full.shape[-2] != 1:\n",
    "            grad_x = grad_x_full.sum(-2)\n",
    "        else:\n",
    "            grad_x = grad_x_full\n",
    "\n",
    "        grad_x = grad_x.view_as(x)\n",
    "\n",
    "        return grad_x, None, None, grad_w, None, grad_w_off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CVMMSel:\n",
    "    raw_sel: torch.Tensor\n",
    "    sel: torch.Tensor\n",
    "    sel_index: torch.Tensor\n",
    "    out_index: Optional[torch.Tensor] = None\n",
    "    reduction_weight: Optional[torch.Tensor] = None\n",
    "\n",
    "    def clone(self) -> 'CVMMSel':\n",
    "        return CVMMSel(self.raw_sel, self.sel, self.sel_index, self.out_index, self.reduction_weight)\n",
    "\n",
    "\n",
    "def cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:\n",
    "    fsel = sel.flatten()\n",
    "    ssel, sel_index = fsel.sort()\n",
    "    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)\n",
    "\n",
    "\n",
    "def cvmm(x: torch.Tensor, sel: Union[torch.Tensor, CVMMSel], keys: torch.Tensor):\n",
    "    if not isinstance(sel, CVMMSel):\n",
    "        sel = cvmm_prepare_sel(sel, keys.shape[0])\n",
    "\n",
    "    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index, sel.reduction_weight)\n",
    "\n",
    "\n",
    "def cvmm_prepare_sel2(sel: torch.Tensor, w: Optional[torch.Tensor] = None) -> CVMMSel:\n",
    "    # Has multiple selections for each batch element\n",
    "    n_per_batch = sel.shape[-1]\n",
    "\n",
    "    # indices = torch.arange(sel.nelement() // n_per_batch, device=sel.device, dtype=torch.int32)\n",
    "    # indices = indices.repeat_interleave(n_per_batch).flatten()\n",
    "\n",
    "    fsel = sel.flatten()\n",
    "    ssel, sel_index = fsel.sort()\n",
    "\n",
    "    # in_index = indices[sel_index]\n",
    "    in_index = sel_index // n_per_batch\n",
    "\n",
    "    return CVMMSel(sel, ssel.view_as(sel), in_index, sel_index, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
